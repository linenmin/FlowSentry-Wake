axelera-model-format: 1.0.0

name: llama-3-2-3b-1024-4core-static

description: HuggingFace Transformers Llama-3.2-3B-Instruct model, limiting the sequence length to 1024 tokens, compiled for 4 cores execution

models:
  llama-3-2-3b-1024-4core-static:
    precompiled_url: https://llm.axelera.ai/icdf/4.x/llama3b_1024_mc_model.tar.gz
    precompiled_path: build/llama-3-2-3b-1024-4core-static/llama3b_1024_mc_model.tar.gz
    precompiled_md5: b679a9d3d203a3c25b50a0f583a45a83
    task_category: LanguageModel
    extra_kwargs:
      llm:
        max_tokens: 1024
        embeddings_url: https://llm.axelera.ai/embeddings/llama_3_2_3b_embeddings.npz
        embeddings_md5: 7f65172bccc306be794c152c06d209b2
        model_name: meta-llama/Llama-3.2-3B-Instruct
        tokenizer_url: https://llm.axelera.ai/embeddings/meta-llama_Llama-3.2-3B-Instruct_tokenizer.zip
        tokenizer_md5: 5064e8e9299c5c41a7cd07093667a613
