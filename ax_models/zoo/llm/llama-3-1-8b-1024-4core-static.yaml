axelera-model-format: 1.0.0

name: llama-3-1-8b-1024-4core-static

description: HuggingFace Transformers Llama-3.1-8B-Instruct model, limiting the sequence length to 1024 tokens, compiled for 4 cores execution

models:
  llama-3-1-8b-1024-4core-static:
    precompiled_url: https://llm.axelera.ai/icdf/4.x/llama8b_1024_mc_model.tar.gz
    precompiled_path: build/llama-3-1-8b-1024-4core-static/llama8b_1024_mc_model.tar.gz
    precompiled_md5: 67962f9dbb5855e42841d1ec82ea5109
    task_category: LanguageModel
    extra_kwargs:
      llm:
        max_tokens: 1024
        embeddings_url: https://llm.axelera.ai/embeddings/llama_3_1_8b_embeddings.npz
        embeddings_md5: adbdc7e5896b8571c92b203f6afedab2
        model_name: meta-llama/Llama-3.1-8B-Instruct
        tokenizer_url: https://llm.axelera.ai/embeddings/meta-llama_Llama-3.1-8B-Instruct_tokenizer.zip
        tokenizer_md5: 6bf2026ce0ed572f87d8ab47a16493ed
        ddr_requirement_gb: 16
